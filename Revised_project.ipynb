{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format: [[word_list], [tag_list]]\n",
    "# word_list = [[tweet1],[tweet2], ...]\n",
    "# tag_list = [[tags for tweet1],[tags for tweet2], ...]\n",
    "# tweet<n> = [word1, word2, ...]\n",
    "# tags for tweet<n> = [tag1, tag2, ...]\n",
    "def read_train(file_name):\n",
    "    in_file = open(file_name,'r',encoding='utf8')\n",
    "    l = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "    for line in in_file:\n",
    "        x = line.strip().split()\n",
    "        if x != []:\n",
    "            words.append(x[0])\n",
    "            tags.append(x[1].rstrip('\\n'))\n",
    "        else:\n",
    "            word_list.append(words)\n",
    "            tag_list.append(tags)\n",
    "            words = []\n",
    "            tags = []\n",
    "\n",
    "    l.append(word_list)\n",
    "    l.append(tag_list)\n",
    "    in_file.close()\n",
    "    return l\n",
    "    \n",
    "# reading and writing to files \n",
    "# format:[[tweet1],[tweet2], ...]\n",
    "# tweet<n> = [word1, word2, ...]\n",
    "def read_dev_in(file_name):\n",
    "    in_file = open(file_name,'r',encoding='utf8')\n",
    "    l = []\n",
    "    tweet = []\n",
    "    for line in in_file:\n",
    "        tweet.append(line.strip())\n",
    "        if line.strip()==\"\":\n",
    "            tweet.remove(\"\")\n",
    "            l.append(tweet)\n",
    "            tweet=[]\n",
    "                \n",
    "        \n",
    "    in_file.close()\n",
    "    return l\n",
    "\n",
    "def write_devp2(language,word_list,tag_list):\n",
    "    file_name = language+\"/\"+\"dev.p2.out\"\n",
    "    if os.path.isfile(file_name):\n",
    "        print('file exist')\n",
    "        try:\n",
    "            os.remove(file_name)\n",
    "            print(\"deleted file\")\n",
    "        except OSError:\n",
    "            print(\"ERROR\")\n",
    "            \n",
    "    out_file = open(file_name,'a',encoding='utf8')\n",
    "\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(len(word_list[i])):\n",
    "            out_file.write(word_list[i][j]+\" \"+tag_list[i][j]+\"\\n\")\n",
    "        out_file.write(\" \\n\")\n",
    "    \n",
    "    out_file.close()\n",
    "        \n",
    "def write_devp3(language,word_list,tag_list):\n",
    "    file_name = language+\"/\"+\"dev.p3a.out\"\n",
    "    if os.path.isfile(file_name):\n",
    "        print('file exist')\n",
    "        try:\n",
    "            os.remove(file_name)\n",
    "            print(\"deleted file\")\n",
    "        except OSError:\n",
    "            print (\"error\")\n",
    "            #pass\n",
    "    out_file = open(file_name,'a',encoding='utf8')\n",
    "\n",
    "    for i in range(len(word_list)): \n",
    "        lw = len(word_list[i])\n",
    "        lt = len(tag_list[i])\n",
    "        #print(\"length of words = \"+str(lw))\n",
    "        #print(\"length of tags = \"+str(lt))\n",
    "        if lt!=lw:\n",
    "            print(word_list[i])\n",
    "            print(tag_list[i])\n",
    "            print (i)\n",
    "        \n",
    "        for j in range(len(word_list[i])):\n",
    "            out_file.write(word_list[i][j]+\" \"+tag_list[i][j]+\"\\n\")\n",
    "        out_file.write(\" \\n\")\n",
    "    \n",
    "    out_file.close()   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time, os, math\n",
    "from collections import Counter\n",
    "from itertools import count\n",
    "\n",
    "class HMM:\n",
    "\n",
    "    tags = [\"B-negative\",\"B-neutral\",\"B-positive\",\"I-negative\",\"I-neutral\",\"I-positive\",\"O\"]\n",
    "    word_tag_em = {} # {(word,tag): emission probability}\n",
    "    \n",
    "    def __init__(self,train_words,train_tags):\n",
    "        self.train_words = train_words \n",
    "        self.train_tags = train_tags \n",
    "        \n",
    "        self.join_train_words = [j for i in train_words for j in i] #convert nested lists into one big list for zipping\n",
    "        self.join_train_tags = [j for i in train_tags for j in i]\n",
    "        \n",
    "        self.tag_count = Counter(self.join_train_tags) # total count for each tag found in training data\n",
    "        self.word_tag_count = Counter(list(zip(self.join_train_words,self.join_train_tags))) # total count for each word-tag pair found in training data\n",
    "        \n",
    "        self.new_word = self.new_word_emission() # (tag,emission)\n",
    "        self.train_emission()\n",
    "        \n",
    "        #print(self.tag_count)\n",
    "        #print(self.word_tag_count)\n",
    "        #print(self.new_word)\n",
    "        #print(self.word_tag_em)\n",
    "#=========================== PART 2 =============================\n",
    "    #counting number of occurence of y_i\n",
    "    def count_tag(self,tag):                \n",
    "        return self.tag_count[tag]\n",
    "    \n",
    "    \n",
    "    #2a count(y pair x)/ count(y)\n",
    "    def emission(self,word,tag):\n",
    "        return self.word_tag_count[(word,tag)]\n",
    "    \n",
    "    \n",
    "    #2b emission count(y pair x)/ (count(y)+1)\n",
    "    def get_emission_prob(self,word,tag):\n",
    "        return self.word_tag_count[(word,tag)]/(self.count_tag(tag)+1)\n",
    "    \n",
    "    #2b finding tag for new word using estimation 1/ (count(y)+1)   \n",
    "    def new_word_emission(self):\n",
    "        em=[]\n",
    "        for tag in self.tags:  \n",
    "            em.append(1/(self.count_tag(tag)+1))\n",
    "        #return label using the index of maximum emission found\n",
    "        new_word_em = max(em)\n",
    "        new_word_tag = self.tags[em.index(max(em))]\n",
    "        return (new_word_tag,new_word_em)\n",
    "    \n",
    "    def train_emission(self):\n",
    "        #for each (word,tag) pair, calculate emission score and put in dictionary\n",
    "        for wt_pair in self.word_tag_count:\n",
    "            self.word_tag_em[wt_pair]=self.get_emission_prob(wt_pair[0],wt_pair[1])\n",
    "        return self.word_tag_em\n",
    "    \n",
    "    def sentiment_analysis(self,test_data):\n",
    "        \n",
    "        predicted_tags = []\n",
    "        for tweet in test_data:\n",
    "            max_tag = []\n",
    "            for word in tweet:\n",
    "                temp = []\n",
    "                if word in self.join_train_words:\n",
    "                    #print(\"\\nI AM INSIDE\\n\")\n",
    "                    for tag in self.tags:\n",
    "                        if (word,tag) in self.word_tag_em:\n",
    "                            temp.append((self.word_tag_em[(word,tag)]))\n",
    "                        else:\n",
    "                            temp.append(0)\n",
    "                    #print(temp)\n",
    "                    max_em = max(temp)\n",
    "                    p_tag = self.tags[temp.index(max(temp))]\n",
    "\n",
    "                else:\n",
    "                    #print(\"\\nI AM OUTSIDE\\n\")\n",
    "                    p_tag = self.new_word[0]\n",
    "                    \n",
    "                if p_tag[0] == \"I\":\n",
    "                    \n",
    "                    if len(max_tag) == 0 or max_tag[-1]==\"O\" or max_tag[-1][1:]!=p_tag[1:]:                           \n",
    "                        p_tag = \"B\"+p_tag[1:]\n",
    "                        \n",
    "                max_tag.append(p_tag)\n",
    "                \n",
    "            predicted_tags.append(max_tag)\n",
    "        return predicted_tags\n",
    "    \n",
    "    \n",
    "                        \n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist\n",
      "deleted file\n",
      "time taken = 1.5312809944152832s\n"
     ]
    }
   ],
   "source": [
    "train_data = read_train(\"EN/train\")\n",
    "test_data = read_dev_in(\"EN/dev.in\")\n",
    "hmm = HMM(train_data[0],train_data[1])\n",
    "starttime = time.time()\n",
    "predicted_tags = hmm.sentiment_analysis(test_data)\n",
    "write_devp2(\"EN\",test_data,predicted_tags)\n",
    "#print(test_data)\n",
    "elapsed = time.time()-starttime\n",
    "\n",
    "print (\"time taken = \"+str(elapsed)+\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
