{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format: [[word_list], [tag_list]]\n",
    "# word_list = [[tweet1],[tweet2], ...]\n",
    "# tag_list = [[tags for tweet1],[tags for tweet2], ...]\n",
    "# tweet<n> = [word1, word2, ...]\n",
    "# tags for tweet<n> = [tag1, tag2, ...]\n",
    "def read_train(file_name):\n",
    "    in_file = open(file_name,'r',encoding='utf8')\n",
    "    l = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "    for line in in_file:\n",
    "        x = line.strip().split()\n",
    "        if x != []:\n",
    "            words.append(x[0].lower())\n",
    "            tags.append(x[1].rstrip('\\n'))\n",
    "        else:\n",
    "            word_list.append(words)\n",
    "            tag_list.append(tags)\n",
    "            words = []\n",
    "            tags = []\n",
    "\n",
    "    l.append(word_list)\n",
    "    l.append(tag_list)\n",
    "    in_file.close()\n",
    "    return l\n",
    "    \n",
    "# reading and writing to files \n",
    "# format:[[tweet1],[tweet2], ...]\n",
    "# tweet<n> = [word1, word2, ...]\n",
    "def read_dev_in(file_name):\n",
    "    in_file = open(file_name,'r',encoding='utf8')\n",
    "    l = []\n",
    "    tweet = []\n",
    "    for line in in_file:\n",
    "        tweet.append(line.strip().lower())\n",
    "        if line.strip()==\"\":\n",
    "            tweet.remove(\"\")\n",
    "            l.append(tweet)\n",
    "            tweet=[]\n",
    "                \n",
    "        \n",
    "    in_file.close()\n",
    "    return l\n",
    "\n",
    "def write_devp5(language,word_list,tag_list):\n",
    "    file_name = language+\"/\"+\"test.p5.out\"\n",
    "    if os.path.isfile(file_name):\n",
    "        print('file exist')\n",
    "        try:\n",
    "            os.remove(file_name)\n",
    "            print(\"deleted file\")\n",
    "        except OSError:\n",
    "            print(\"ERROR\")\n",
    "            \n",
    "    out_file = open(file_name,'a',encoding='utf8')\n",
    "\n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(len(word_list[i])):\n",
    "            out_file.write(word_list[i][j]+\" \"+tag_list[i][j]+\"\\n\")\n",
    "        out_file.write(\" \\n\")\n",
    "    \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check if string is made up of numbers (will not affect sentiment)\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "#list of stop words to disregard in tweet\n",
    "#stop_file = open(\"stop_list.txt\",'r',encoding='utf8')\n",
    "stop_file = open(\"stop_list_es.txt\",'r')\n",
    "stop_list = []\n",
    "for line in stop_file:\n",
    "    if line[0] != \"\\ufeff\":\n",
    "        stop_list.append(line.strip())                \n",
    "        \n",
    "stop_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, itertools, re, math, operator, os\n",
    "from collections import Counter\n",
    "\n",
    "class part5:\n",
    "    tags = [\"B-negative\",\"B-neutral\",\"B-positive\",\"I-negative\",\"I-neutral\",\"I-positive\",\"O\"]\n",
    "    \n",
    "    def __init__(self,train_words,train_tags,stop_list):\n",
    "        self.train_words = train_words\n",
    "        self.train_tags = train_tags\n",
    "        self.long_train_words =  [j for i in self.train_words for j in i]\n",
    "        self.stop_list = stop_list\n",
    "        self.word_dict = {}\n",
    "        self.preprocessing(train_words,train_tags)\n",
    "        processed_words = [j for i in self.p_words_list for j in i]\n",
    "        \n",
    "        self.word_count = Counter(processed_words)\n",
    "\n",
    "        \n",
    "    def preprocessing(self,train_words,train_tags):\n",
    "        self.p_words_list = []\n",
    "        self.p_tags_list = []\n",
    "\n",
    "        for tweet_index in range(len(train_words)):\n",
    "            tweet_word = train_words[tweet_index]\n",
    "            tweet_tag = train_tags[tweet_index]\n",
    "            processed_word = []\n",
    "            processed_tag = []\n",
    "            \n",
    "            for word_index in range(len(tweet_word)):\n",
    "                word = tweet_word[word_index]\n",
    "                tag = tweet_tag[word_index]\n",
    "                \n",
    "                #removing unnecessary words eg stop words, urls\n",
    "                if word[0] == \"@\" or word[0] == \"#\":\n",
    "                    word = word[1:]\n",
    "                elif word not in self.stop_list and word[0:7]!=\"http://\" and word.isalnum() and not(is_number(word)):\n",
    "                    if len(word)>=5:\n",
    "                        word = self.word_stem(word)\n",
    "                    elif len(word)>=4:\n",
    "                        word = self.remove_repeat(word)\n",
    "                    if word not in self.word_dict:\n",
    "                        self.word_dict[word]= {\"B-negative\":0,\"B-neutral\":0,\"B-positive\":0,\"I-negative\":0,\"I-neutral\":0,\"I-positive\":0,\"O\":0}\n",
    "                    self.word_dict[word][tag]+=1\n",
    "\n",
    "                    #print(self.word_stem(word))\n",
    "                    processed_word.append(word)\n",
    "                    processed_tag.append(tag)\n",
    "                tag_group = []\n",
    "                word_group = []\n",
    "                \n",
    "                \"\"\"for tag_i in range(len(processed_tag)):\n",
    "                    tag = processed_tag[tag_i]\n",
    "                    if tag[0] == \"B\" and tag_group == []:\n",
    "                        tag_group.append(tag)\n",
    "                        word_group.append(processed_word[tag_i])\n",
    "                        \n",
    "                    elif tag[0] == \"B\" and tag_group != []:\n",
    "                        phrase = \" \".join(word_group)\n",
    "                        \n",
    "                        if phrase not in self.word_dict:\n",
    "                            sentiment = tag_group[-1][1:]\n",
    "                            self.word_dict[phrase] = {\"B-negative\":0,\"B-neutral\":0,\"B-positive\":0,\"I-negative\":0,\"I-neutral\":0,\"I-positive\":0,\"O\":0}\n",
    "                        self.word_dict[phrase][\"B\"+sentiment] += 1\n",
    "                        word_group = []\n",
    "                        tag_group = []\n",
    "                    elif tag[0] == \"I\":\n",
    "                        tag_group.append(tag)\n",
    "                        word_group.append(processed_word[tag_i])\"\"\"\n",
    "                    \n",
    "                        \n",
    "                                        \n",
    "            self.p_words_list.append(processed_word)\n",
    "            self.p_tags_list.append(processed_tag)\n",
    "        \n",
    "    #removes repeated characters in a word if character repeats more than 2 times for example haaapppyyy -> haappyy\n",
    "    def remove_repeat(self,word):\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    \n",
    "    #attempt to reduce words to their root words for example going -> go\n",
    "    def word_stem(self,word):\n",
    "        \n",
    "        n = len(word)\n",
    "        \n",
    "        if word[n-3:] == \"ing\" and word[:n-3] in self.long_train_words:\n",
    "            new_word = word[:n-3]          \n",
    "            return new_word\n",
    "        \n",
    "        elif word[n-2:] == \"ed\" and word[:n-2] in self.long_train_words:\n",
    "            new_word = word[:n-2]\n",
    "            return new_word\n",
    "        \n",
    "        elif word[n-1] == \"s\" and word[:n-1] in self.long_train_words:\n",
    "            new_word = word[:n-1]\n",
    "            return new_word\n",
    "        \n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    #to reduce data that causes skewed prediction\n",
    "    def reduce_dict(self):\n",
    "        list_sum = []\n",
    "        for word in self.word_dict:\n",
    "            total = sum(self.word_dict[word].values())\n",
    "            list_sum.append((total,word))\n",
    "        for i in range(len(list_sum)):\n",
    "            if list_sum[i][0]<3:\n",
    "                self.word_dict.pop(word,None)\n",
    "        return list_sum\n",
    "    \n",
    "    #train naive bayes\n",
    "    def nb_training(self):\n",
    "        count_label = {}\n",
    "        for tag in self.tags:\n",
    "            count_label[tag] = 0\n",
    "            for word in self.word_dict:\n",
    "                count_label[tag] += self.word_dict[word][tag]\n",
    "        total = sum(count_label.values())\n",
    "        count_label[\"prob\"]={}\n",
    "        for tag in self.tags:\n",
    "            count_label[\"prob\"][tag] = count_label[tag]/total\n",
    "        return count_label\n",
    "            \n",
    "    #run naive bayes algorithm    \n",
    "    def naive_bayes(self,test_data):\n",
    "        train_result = self.nb_training()\n",
    "        predicted_result= []\n",
    "        \n",
    "        for tweet in test_data:\n",
    "            tweet_sentiment = []\n",
    "            predicted_tag = []\n",
    "            prob = 0\n",
    "            for tag in self.tags:\n",
    "                prob = math.log(train_result[\"prob\"][tag])-math.log(len(tweet)*train_result[tag])\n",
    "                \n",
    "                for word in tweet:\n",
    "                                            \n",
    "                    if word in self.word_dict:\n",
    "                        occurence = self.word_dict[word][tag]\n",
    "                        if occurence > 0:\n",
    "                            prob+=math.log(occurence)\n",
    "                        else:\n",
    "                            prob+=math.log(1)\n",
    "                    else:\n",
    "                        prob+=math.log(1)\n",
    "                tweet_sentiment.append((prob,tag))\n",
    "            tweet_sentiment.sort()\n",
    "            most_probable_sentiment = tweet_sentiment[-1][1]\n",
    "            \n",
    "            for word in tweet:\n",
    "                if word[0] == \"#\" or word[0] == \"@\":\n",
    "                    if len(word)>=5:\n",
    "                        new_word = self.word_stem(word[1:])\n",
    "                    else:\n",
    "                        new_word = word[1:]\n",
    "                    if new_word in self.word_dict:\n",
    "                        \n",
    "                        new_tag = max(self.word_dict[new_word].items(), key=operator.itemgetter(1))[0]\n",
    "                        predicted_tag.append(new_tag)\n",
    "                    else:\n",
    "                        predicted_tag.append(most_probable_sentiment)\n",
    "                elif word in self.stop_list or word[0:7]==\"http://\" or not word.isalnum() or is_number(word):\n",
    "                    predicted_tag.append(\"O\")\n",
    "                else:\n",
    "                    new_word = self.word_stem(word)\n",
    "                    if new_word in self.word_dict:\n",
    "                        \n",
    "                        new_tag = max(self.word_dict[new_word].items(), key=operator.itemgetter(1))[0]\n",
    "                        \n",
    "                        predicted_tag.append(new_tag)\n",
    "                    else:\n",
    "                        predicted_tag.append(most_probable_sentiment)\n",
    "            predicted_result.append(predicted_tag)\n",
    "        return predicted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "file exist\n",
      "deleted file\n",
      "time taken = 2.60986065864563s\n"
     ]
    }
   ],
   "source": [
    "train_data = read_train(\"ES/train\")\n",
    "test_data = read_dev_in(\"ES/test.in\")\n",
    "#train_data = read_train(\"EN/train\")\n",
    "#test_data = read_dev_in(\"EN/test.in\")\n",
    "\n",
    "starttime = time.time()\n",
    "part_5 = part5(train_data[0],train_data[1],stop_list)\n",
    "print(\"predicting...\")\n",
    "predicted_tags = part_5.naive_bayes(test_data)\n",
    "\n",
    "write_devp5(\"ES\",test_data,predicted_tags)\n",
    "#write_devp5(\"EN\",test_data,predicted_tags)\n",
    "\n",
    "elapsed = time.time()-starttime\n",
    "\n",
    "print (\"time taken = \"+str(elapsed)+\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
