{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, itertools, re, math, operator, os\n",
    "from collections import Counter\n",
    "\n",
    "class part5:\n",
    "    tags = [\"negative\",\"neutral\",\"positive\",\"O\"]\n",
    "    \n",
    "    def __init__(self,train_words,train_tags):\n",
    "        self.long_train_words = [j for i in train_words for j in i]\n",
    "        new_tags = self.convert_tag(train_tags)\n",
    "        processed_train = self.get_processed_tweets(train_words)\n",
    "        #print(processed_train)\n",
    "        self.word_tag_count = self.get_word_tag_count(processed_train,new_tags)\n",
    "        #print(self.word_tag_count)\n",
    "        self.generate_entities(train_words,train_tags)\n",
    "        tweet_sentiment = self.get_tweet_sentiment(new_tags)\n",
    "        self.generate_bigrams(processed_train,tweet_sentiment)\n",
    "        #print(len(self.word_tag_count))\n",
    "        self.generate_word_class_count(train_words,tweet_sentiment)\n",
    "        self.reduce_dict()\n",
    "    #converting all tags to just their sentiments (ignoring \"B-\" and \"I-\") to accumulate count for classification purposes\n",
    "    def convert_tag(self,train_tags):\n",
    "        new_tag_list = []\n",
    "        for tweet in train_tags:\n",
    "            l = []\n",
    "            for tag in tweet:\n",
    "                if tag != \"O\":\n",
    "                    l.append(tag[2:])\n",
    "                else:\n",
    "                    l.append(\"O\")\n",
    "            new_tag_list.append(l)\n",
    "        return new_tag_list\n",
    "    \n",
    "    def remove_repeat(self,word):\n",
    "        if len(word)>3:\n",
    "            return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    def word_stem(self,word):        \n",
    "        n = len(word)\n",
    "        if n>4:\n",
    "            if word[n-3:] == \"ing\" and word[:n-3] in self.long_train_words:\n",
    "                #print(\"ing!\")\n",
    "                new_word = word[:n-3]          \n",
    "                return new_word\n",
    "            elif word[n-2:] == \"ed\" and word[:n-2] in self.long_train_words:\n",
    "                #print(\"ed!\")\n",
    "                new_word = word[:n-2]\n",
    "                return new_word\n",
    "            elif word[n-1] == \"s\" and word[:n-1] in self.long_train_words:\n",
    "                #print(\"s!\")\n",
    "                new_word = word[:n-1]\n",
    "                return new_word\n",
    "            else:\n",
    "                return word\n",
    "        else:\n",
    "            return word\n",
    "            \n",
    "    #remove numbers, special characters, urls and converts \n",
    "    def get_processed_tweets(self,train_words):\n",
    "        processed_tweets = []\n",
    "        for tweet in train_words:\n",
    "            p_tweet = []\n",
    "            for word in tweet:\n",
    "                \n",
    "                if word in stop_list:\n",
    "                    new_word = \"*!__STOP__!*\"\n",
    "                    \n",
    "                elif word in negate_list:\n",
    "                    new_word = \"*!__NEGATIVE__!\"\n",
    "                \n",
    "                elif word == \"rt\":\n",
    "                    new_word = \"*!__RT__!*\"\n",
    "                elif (word[0] == \"#\" or word[0] == \"@\") and len(word)>1:\n",
    "                    new_word = self.word_stem(self.remove_repeat(word[1:]))\n",
    "                    \n",
    "                elif word[0:7]==\"http://\":\n",
    "                    new_word = \"*!__URL__!*\"\n",
    "                    \n",
    "                elif not word.isalnum():\n",
    "                    new_word = \"*!__SPECIAL__!*\"\n",
    "                    \n",
    "                elif is_number(word):\n",
    "                    new_word = \"*!__NUM__!*\"\n",
    "                \n",
    "                else:\n",
    "                    new_word = self.word_stem(self.remove_repeat(word))\n",
    "\n",
    "                p_tweet.append(new_word)\n",
    "            processed_tweets.append(p_tweet)\n",
    "        return processed_tweets\n",
    "                \n",
    "    def generate_entities(self,train_words,train_tags):\n",
    "        for i in range(len(train_tags)):           \n",
    "            phrase_list = []\n",
    "            phrase_tag = \"\"\n",
    "            for j in range(len(train_tags[i])-1):\n",
    "                word = train_words[i][j]\n",
    "                current_tag = train_tags[i][j]\n",
    "                next_tag = train_tags[i][j+1] \n",
    "                \n",
    "                if current_tag[0] == \"B\":\n",
    "                    phrase_list.append(word)\n",
    "                    phrase_tag = current_tag[2:]\n",
    "                    \n",
    "                elif current_tag[0] == \"I\":\n",
    "                    phrase_list.append(word)\n",
    "                    if next_tag == \"O\" or next_tag[0] == \"B\":\n",
    "                        if len(phrase_list)>1:\n",
    "                            phrase = \" \".join(phrase_list)\n",
    "                            if phrase in self.word_tag_count:\n",
    "                                self.word_tag_count[phrase][phrase_tag] +=1\n",
    "                            else:\n",
    "                                self.word_tag_count[phrase] = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}\n",
    "                                self.word_tag_count[phrase][phrase_tag]+=1\n",
    "                        phrase_list=[]\n",
    "                        phrase_tag=\"\"\n",
    "                        \n",
    "    def get_tweet_sentiment(self,new_tags):\n",
    "        tweet_sentiment = []\n",
    "        \n",
    "        for tweet in new_tags:\n",
    "            count_tag = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}\n",
    "            for tag in tweet:\n",
    "                if tag != \"O\":\n",
    "                    count_tag[tag]+=1\n",
    "            \n",
    "            \n",
    "            sent = max(count_tag.items(), key=operator.itemgetter(1))[0]\n",
    "            \n",
    "            tweet_sentiment.append(sent)\n",
    "        return tweet_sentiment\n",
    "    \n",
    "    def generate_bigrams(self,processed_words,tweet_sentiment):\n",
    "        check_worthy = {}\n",
    "        for i in range(len(processed_words)):\n",
    "            for j in range(len(processed_words[i])-1):\n",
    "                current_word = processed_words[i][j]\n",
    "                next_word = processed_words[i][j-1]\n",
    "                if current_word in negate_list:\n",
    "                    if tweet_sentiment[i] == \"positive\":\n",
    "                        self.word_tag_count[next_word][\"negative\"] +=1\n",
    "                    elif tweet_sentiment[i] == \"negative\":\n",
    "                        self.word_tag_count[next_word][\"positive\"] +=1\n",
    "                        \n",
    "                if current_word[:4]!=\"*!__\" and next_word[:4]!=\"*!__\":\n",
    "                    phrase = current_word+\" \"+next_word\n",
    "\n",
    "                    if phrase in check_worthy:\n",
    "                        check_worthy[phrase][tweet_sentiment[i]] +=1\n",
    "                    else:\n",
    "                        check_worthy[phrase] = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}                    \n",
    "                        check_worthy[phrase][tweet_sentiment[i]] +=1\n",
    "        for word in check_worthy:\n",
    "            total = sum(check_worthy[word].values())\n",
    "            if total>1:\n",
    "                if word not in self.word_tag_count:                    \n",
    "                    self.word_tag_count[word]=check_worthy[word]\n",
    "                        \n",
    "                                        \n",
    "                    \n",
    "    def get_word_tag_count(self,processed_words,new_tags):\n",
    "        word_tag_count = {}\n",
    "        for i in range(len(processed_words)):\n",
    "            for j in range(len(processed_words[i])):\n",
    "                word = processed_words[i][j]\n",
    "                tag = new_tags[i][j]\n",
    "                \n",
    "                if word in word_tag_count:\n",
    "                    word_tag_count[word][tag] +=1\n",
    "                else:\n",
    "                    word_tag_count[word] = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}\n",
    "                    word_tag_count[word][tag]+=1\n",
    "        return word_tag_count\n",
    "    \n",
    "    def generate_word_class_count(self,train_words,tweet_sentiment):\n",
    "        self.class_count = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}\n",
    "        self.word_class_count = {}\n",
    "        for i in range(len(train_words)):\n",
    "            tweet = train_words[i]\n",
    "            sentiment = tweet_sentiment[i]\n",
    "            count = Counter(tweet)\n",
    "            self.class_count[sentiment] += sum(count.values())\n",
    "            for word in count:\n",
    "                if word not in self.word_class_count:\n",
    "                    self.word_class_count[word] = {\"negative\":0, \"neutral\":0, \"positive\":0, \"O\":0}\n",
    "                self.word_class_count[word][sentiment] += count[word]\n",
    "        #print(self.word_class_count)\n",
    "    def reduce_dict(self):\n",
    "        for word in self.long_train_words:\n",
    "            if word in self.word_class_count:\n",
    "                total = sum(self.word_class_count[word].values())\n",
    "                if total<3:\n",
    "                    self.word_class_count.pop(word,None)\n",
    "                    \n",
    "    def score(self,sentiment,tweet):\n",
    "        total_num = math.log(sum(self.class_count.values()))\n",
    "        print(self.class_count)\n",
    "        #print(self.word_class_count)\n",
    "        sentiment_count = math.log(self.class_count[sentiment])\n",
    "        #number of words with label c / total numer of words\n",
    "        prob_sentiment = sentiment_count-total_num\n",
    "        #product(of word(i) occurence how many are labelled c / number of words with label c)\n",
    "        product = 0\n",
    "        for word in tweet:    \n",
    "            if word in self.word_class_count:\n",
    "                word_occurence = self.word_class_count[word][sentiment]\n",
    "                if word_occurence!=0:\n",
    "                    word_occurence = math.log(word_occurence)\n",
    "                else:\n",
    "                    word_occurence = 0\n",
    "                product += word_occurence-sentiment_count\n",
    "            else:\n",
    "                product += -sentiment_count\n",
    "                \n",
    "        return prob_sentiment+product\n",
    "    \n",
    "   \n",
    "    def naive_bayes(self,test_data):\n",
    "        sentiments = [\"negative\",\"neutral\",\"positive\"]\n",
    "        total_sentiments = []\n",
    "        p_tags=[]\n",
    "        for tweet in test_data:\n",
    "            tweet_sentiment = []\n",
    "            predicted_tag = []\n",
    "            prob = 0\n",
    "            sentiments = [\"negative\",\"neutral\",\"positive\"]\n",
    "            #prob of tweet being this tag\n",
    "            score = 0\n",
    "            for sentiment in sentiments:\n",
    "                score+=self.score(sentiment,tweet)\n",
    "            \n",
    "            for p_sen in sentiments:\n",
    "                #print(self.score(p_sen,tweet))\n",
    "                prob_c = self.score(p_sen,tweet)/score\n",
    "                tweet_sentiment.append((prob_c,p_sen))\n",
    "            tweet_sent = (max(tweet_sentiment))[1]\n",
    "            total_sentiments.append(tweet_sent)\n",
    "        processed_test = self.get_processed_tweets(test_data)\n",
    "        \n",
    "        for i in range(len(processed_test)):\n",
    "            tweet = processed_test[i]\n",
    "            predict_tag = []\n",
    "            for word in tweet:\n",
    "                if word[:4] == \"*!__\":\n",
    "                    predict_tag.append(\"O\")\n",
    "                elif word in self.word_tag_count:\n",
    "                    maxim_tag = max(self.word_tag_count[word].items(), key=operator.itemgetter(1))[0]\n",
    "                    predict_tag.append(\"B-\"+maxim_tag)\n",
    "                else:\n",
    "                    predict_tag.append(total_sentiments[i])\n",
    "            p_tags.append(predict_tag)\n",
    "        return p_tags\n",
    "                    \n",
    "                \n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
